{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7bca4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:90% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 분할을 위한 폴더 생성\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "original_dataset_dir='C:/Users/user/Desktop/kang/dataset'\n",
    "classes_list = os.listdir(original_dataset_dir)\n",
    "\n",
    "base_dir='C:/Users/user/Desktop/kang/spliteed'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "train_dir=os.path.join(base_dir,'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir=os.path.join(base_dir,'val')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir=os.path.join(base_dir,'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "for clss in classes_list:\n",
    "    os.mkdir(os.path.join(train_dir,clss))\n",
    "    os.mkdir(os.path.join(validation_dir,clss))\n",
    "    os.mkdir(os.path.join(test_dir,clss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7311e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size( Apple___Apple_scab ):  378\n",
      "Validation size( Apple___Apple_scab ):  126\n",
      "Test size( Apple___Apple_scab ):  126\n",
      "Train size( Apple___Black_rot ):  372\n",
      "Validation size( Apple___Black_rot ):  124\n",
      "Test size( Apple___Black_rot ):  124\n",
      "Train size( Apple___Cedar_apple_rust ):  165\n",
      "Validation size( Apple___Cedar_apple_rust ):  55\n",
      "Test size( Apple___Cedar_apple_rust ):  55\n",
      "Train size( Apple___healthy ):  987\n",
      "Validation size( Apple___healthy ):  329\n",
      "Test size( Apple___healthy ):  329\n",
      "Train size( Cherry___healthy ):  512\n",
      "Validation size( Cherry___healthy ):  170\n",
      "Test size( Cherry___healthy ):  170\n",
      "Train size( Cherry___Powdery_mildew ):  631\n",
      "Validation size( Cherry___Powdery_mildew ):  210\n",
      "Test size( Cherry___Powdery_mildew ):  210\n",
      "Train size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  307\n",
      "Validation size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Test size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Train size( Corn___Common_rust ):  715\n",
      "Validation size( Corn___Common_rust ):  238\n",
      "Test size( Corn___Common_rust ):  238\n",
      "Train size( Corn___healthy ):  697\n",
      "Validation size( Corn___healthy ):  232\n",
      "Test size( Corn___healthy ):  232\n",
      "Train size( Corn___Northern_Leaf_Blight ):  591\n",
      "Validation size( Corn___Northern_Leaf_Blight ):  197\n",
      "Test size( Corn___Northern_Leaf_Blight ):  197\n",
      "Train size( Grape___Black_rot ):  708\n",
      "Validation size( Grape___Black_rot ):  236\n",
      "Test size( Grape___Black_rot ):  236\n",
      "Train size( Grape___Esca_(Black_Measles) ):  829\n",
      "Validation size( Grape___Esca_(Black_Measles) ):  276\n",
      "Test size( Grape___Esca_(Black_Measles) ):  276\n",
      "Train size( Grape___healthy ):  253\n",
      "Validation size( Grape___healthy ):  84\n",
      "Test size( Grape___healthy ):  84\n",
      "Train size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  645\n",
      "Validation size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
      "Test size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
      "Train size( Peach___Bacterial_spot ):  1378\n",
      "Validation size( Peach___Bacterial_spot ):  459\n",
      "Test size( Peach___Bacterial_spot ):  459\n",
      "Train size( Peach___healthy ):  216\n",
      "Validation size( Peach___healthy ):  72\n",
      "Test size( Peach___healthy ):  72\n",
      "Train size( Pepper,_bell___Bacterial_spot ):  598\n",
      "Validation size( Pepper,_bell___Bacterial_spot ):  199\n",
      "Test size( Pepper,_bell___Bacterial_spot ):  199\n",
      "Train size( Pepper,_bell___healthy ):  886\n",
      "Validation size( Pepper,_bell___healthy ):  295\n",
      "Test size( Pepper,_bell___healthy ):  295\n",
      "Train size( Potato___Early_blight ):  600\n",
      "Validation size( Potato___Early_blight ):  200\n",
      "Test size( Potato___Early_blight ):  200\n",
      "Train size( Potato___healthy ):  91\n",
      "Validation size( Potato___healthy ):  30\n",
      "Test size( Potato___healthy ):  30\n",
      "Train size( Potato___Late_blight ):  600\n",
      "Validation size( Potato___Late_blight ):  200\n",
      "Test size( Potato___Late_blight ):  200\n",
      "Train size( Strawberry___healthy ):  273\n",
      "Validation size( Strawberry___healthy ):  91\n",
      "Test size( Strawberry___healthy ):  91\n",
      "Train size( Strawberry___Leaf_scorch ):  665\n",
      "Validation size( Strawberry___Leaf_scorch ):  221\n",
      "Test size( Strawberry___Leaf_scorch ):  221\n",
      "Train size( Tomato___Bacterial_spot ):  1276\n",
      "Validation size( Tomato___Bacterial_spot ):  425\n",
      "Test size( Tomato___Bacterial_spot ):  425\n",
      "Train size( Tomato___Early_blight ):  600\n",
      "Validation size( Tomato___Early_blight ):  200\n",
      "Test size( Tomato___Early_blight ):  200\n",
      "Train size( Tomato___healthy ):  954\n",
      "Validation size( Tomato___healthy ):  318\n",
      "Test size( Tomato___healthy ):  318\n",
      "Train size( Tomato___Late_blight ):  1145\n",
      "Validation size( Tomato___Late_blight ):  381\n",
      "Test size( Tomato___Late_blight ):  381\n",
      "Train size( Tomato___Leaf_Mold ):  571\n",
      "Validation size( Tomato___Leaf_Mold ):  190\n",
      "Test size( Tomato___Leaf_Mold ):  190\n",
      "Train size( Tomato___Septoria_leaf_spot ):  1062\n",
      "Validation size( Tomato___Septoria_leaf_spot ):  354\n",
      "Test size( Tomato___Septoria_leaf_spot ):  354\n",
      "Train size( Tomato___Spider_mites Two-spotted_spider_mite ):  1005\n",
      "Validation size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
      "Test size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
      "Train size( Tomato___Target_Spot ):  842\n",
      "Validation size( Tomato___Target_Spot ):  280\n",
      "Test size( Tomato___Target_Spot ):  280\n",
      "Train size( Tomato___Tomato_mosaic_virus ):  223\n",
      "Validation size( Tomato___Tomato_mosaic_virus ):  74\n",
      "Test size( Tomato___Tomato_mosaic_virus ):  74\n",
      "Train size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  3214\n",
      "Validation size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n",
      "Test size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n"
     ]
    }
   ],
   "source": [
    "#데이터 분할과 클래스별 데이터 수 확인\n",
    "import math\n",
    "\n",
    "for clss in classes_list:\n",
    "    path=os.path.join(original_dataset_dir,clss)\n",
    "    fnames=os.listdir(path)\n",
    "    \n",
    "    train_size=math.floor(len(fnames) * 0.6)\n",
    "    validation_size=math.floor(len(fnames) * 0.2)\n",
    "    test_size=math.floor(len(fnames)* 0.2)\n",
    "    \n",
    "    train_fnames=fnames[:train_size]\n",
    "    print('Train size(',clss,'): ',len(train_fnames))\n",
    "    for fname in train_fnames:\n",
    "        src=os.path.join(path, fname)\n",
    "        dst=os.path.join(os.path.join(train_dir, clss), fname)\n",
    "        shutil.copyfile(src,dst)\n",
    "    \n",
    "    validation_fnames=fnames[train_size:(validation_size + train_size)]\n",
    "    print('Validation size(',clss,'): ',len(validation_fnames))\n",
    "    for fname in validation_fnames:\n",
    "        src=os.path.join(path, fname)\n",
    "        dst=os.path.join(os.path.join(validation_dir, clss), fname)\n",
    "        shutil.copyfile(src,dst)\n",
    "    \n",
    "    test_fnames=fnames[(train_size + validation_size):(validation_size + train_size + test_size)]\n",
    "    print('Test size(',clss,'): ', len(test_fnames))\n",
    "    for fname in test_fnames:\n",
    "        src=os.path.join(path,fname)\n",
    "        dst=os.path.join(os.path.join(test_dir,clss),fname)\n",
    "        shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f77e705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#베이스라인 모델 학습을 위한 준비\n",
    "import torch\n",
    "\n",
    "USE_CUDA=torch.cuda.is_available()\n",
    "DEVICE=torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "BATCH_SIZE =256\n",
    "EPOCH=30\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "transform_base=transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = ImageFolder(root='C:/Users/user/Desktop/kang/spliteed/train', transform=transform_base)\n",
    "val_dataset = ImageFolder(root='C:/Users/user/Desktop/kang/spliteed/val', transform=transform_base)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader=torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a21195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#베이스라인 모델 설계\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.conv1=nn.Conv2d(3,32,3,padding=1)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(32,64,3,padding=1)\n",
    "        self.conv3=nn.Conv2d(64,64,3,padding=1)\n",
    "        \n",
    "        self.fc1=nn.Linear(4096,512)\n",
    "        self.fc2=nn.Linear(512,33)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x=self.conv1(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.pool(x)\n",
    "        x=F.dropout(x, p=0.25, training=self.training)\n",
    "        \n",
    "        x=self.conv2(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.pool(x)\n",
    "        x=F.dropout(x, p=0.25, training=self.training)\n",
    "        \n",
    "        x=self.conv3(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.pool(x)\n",
    "        x=F.dropout(x, p=0.25, training=self.training)\n",
    "        \n",
    "        x=x.view(-1,4096)\n",
    "        x=self.fc1(x)\n",
    "        x=F.relu(x)\n",
    "        x=F.dropout(x, p=0.5, training=self.training)\n",
    "        x=self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x,dim=1)\n",
    "    \n",
    "model_base=Net().to(DEVICE)\n",
    "optimizer=optim.Adam(model_base.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "709d4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습을 위한 함수\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_indx, (data,target) in enumerate(train_loader):\n",
    "        data, target=data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(data)\n",
    "        loss=F.cross_entropy(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "180a741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 평가를 위한 함수\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data,target=data.to(DEVICE), target.to(DEVICE)\n",
    "            output=model(data)\n",
    "            \n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            \n",
    "            pred=output.max(1,keepdim=True)[1]\n",
    "            correct+=pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /=len(test_loader.dataset)\n",
    "    test_accuracy = 100 * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e84f2073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "train Loss: 1.9976, Accuracy: 43.34%\n",
      "val Loss: 2.0098, Accuracy: 42.93%\n",
      "Completed in 1m 17s\n",
      "epoch 2\n",
      "train Loss: 1.1228, Accuracy: 66.16%\n",
      "val Loss: 1.1623, Accuracy: 65.15%\n",
      "Completed in 0m 49s\n",
      "epoch 3\n",
      "train Loss: 0.8093, Accuracy: 75.47%\n",
      "val Loss: 0.8603, Accuracy: 73.90%\n",
      "Completed in 0m 50s\n",
      "epoch 4\n",
      "train Loss: 0.6350, Accuracy: 80.43%\n",
      "val Loss: 0.7026, Accuracy: 78.56%\n",
      "Completed in 0m 49s\n",
      "epoch 5\n",
      "train Loss: 0.6286, Accuracy: 79.74%\n",
      "val Loss: 0.6989, Accuracy: 77.02%\n",
      "Completed in 0m 49s\n",
      "epoch 6\n",
      "train Loss: 0.5099, Accuracy: 83.37%\n",
      "val Loss: 0.5865, Accuracy: 80.59%\n",
      "Completed in 0m 49s\n",
      "epoch 7\n",
      "train Loss: 0.4041, Accuracy: 87.20%\n",
      "val Loss: 0.4912, Accuracy: 84.23%\n",
      "Completed in 0m 49s\n",
      "epoch 8\n",
      "train Loss: 0.3733, Accuracy: 88.49%\n",
      "val Loss: 0.4634, Accuracy: 85.58%\n",
      "Completed in 0m 48s\n",
      "epoch 9\n",
      "train Loss: 0.3025, Accuracy: 91.16%\n",
      "val Loss: 0.3935, Accuracy: 87.70%\n",
      "Completed in 0m 49s\n",
      "epoch 10\n",
      "train Loss: 0.2985, Accuracy: 90.60%\n",
      "val Loss: 0.4013, Accuracy: 86.78%\n",
      "Completed in 0m 48s\n",
      "epoch 11\n",
      "train Loss: 0.2999, Accuracy: 90.54%\n",
      "val Loss: 0.3992, Accuracy: 87.11%\n",
      "Completed in 0m 49s\n",
      "epoch 12\n",
      "train Loss: 0.2374, Accuracy: 92.91%\n",
      "val Loss: 0.3454, Accuracy: 88.56%\n",
      "Completed in 0m 48s\n",
      "epoch 13\n",
      "train Loss: 0.2051, Accuracy: 94.08%\n",
      "val Loss: 0.3115, Accuracy: 89.90%\n",
      "Completed in 0m 48s\n",
      "epoch 14\n",
      "train Loss: 0.1804, Accuracy: 94.70%\n",
      "val Loss: 0.2976, Accuracy: 90.27%\n",
      "Completed in 0m 48s\n",
      "epoch 15\n",
      "train Loss: 0.1784, Accuracy: 94.91%\n",
      "val Loss: 0.2966, Accuracy: 90.42%\n",
      "Completed in 0m 49s\n",
      "epoch 16\n",
      "train Loss: 0.1538, Accuracy: 95.70%\n",
      "val Loss: 0.2678, Accuracy: 91.44%\n",
      "Completed in 0m 48s\n",
      "epoch 17\n",
      "train Loss: 0.1755, Accuracy: 94.83%\n",
      "val Loss: 0.3003, Accuracy: 90.30%\n",
      "Completed in 0m 49s\n",
      "epoch 18\n",
      "train Loss: 0.1991, Accuracy: 93.76%\n",
      "val Loss: 0.3297, Accuracy: 89.36%\n",
      "Completed in 0m 49s\n",
      "epoch 19\n",
      "train Loss: 0.1536, Accuracy: 95.61%\n",
      "val Loss: 0.2886, Accuracy: 90.41%\n",
      "Completed in 0m 49s\n",
      "epoch 20\n",
      "train Loss: 0.1432, Accuracy: 95.82%\n",
      "val Loss: 0.2712, Accuracy: 90.87%\n",
      "Completed in 0m 49s\n",
      "epoch 21\n",
      "train Loss: 0.1071, Accuracy: 97.15%\n",
      "val Loss: 0.2393, Accuracy: 92.48%\n",
      "Completed in 0m 49s\n",
      "epoch 22\n",
      "train Loss: 0.1257, Accuracy: 96.16%\n",
      "val Loss: 0.2628, Accuracy: 91.26%\n",
      "Completed in 0m 49s\n",
      "epoch 23\n",
      "train Loss: 0.0941, Accuracy: 97.50%\n",
      "val Loss: 0.2352, Accuracy: 92.36%\n",
      "Completed in 0m 49s\n",
      "epoch 24\n",
      "train Loss: 0.1024, Accuracy: 97.03%\n",
      "val Loss: 0.2478, Accuracy: 91.95%\n",
      "Completed in 0m 49s\n",
      "epoch 25\n",
      "train Loss: 0.0876, Accuracy: 97.44%\n",
      "val Loss: 0.2360, Accuracy: 92.50%\n",
      "Completed in 0m 49s\n",
      "epoch 26\n",
      "train Loss: 0.0759, Accuracy: 98.01%\n",
      "val Loss: 0.2193, Accuracy: 92.98%\n",
      "Completed in 0m 49s\n",
      "epoch 27\n",
      "train Loss: 0.0701, Accuracy: 98.27%\n",
      "val Loss: 0.2136, Accuracy: 92.97%\n",
      "Completed in 0m 48s\n",
      "epoch 28\n",
      "train Loss: 0.0605, Accuracy: 98.62%\n",
      "val Loss: 0.2027, Accuracy: 93.59%\n",
      "Completed in 0m 48s\n",
      "epoch 29\n",
      "train Loss: 0.0626, Accuracy: 98.54%\n",
      "val Loss: 0.1990, Accuracy: 93.53%\n",
      "Completed in 0m 48s\n",
      "epoch 30\n",
      "train Loss: 0.0589, Accuracy: 98.50%\n",
      "val Loss: 0.2140, Accuracy: 93.34%\n",
      "Completed in 0m 48s\n"
     ]
    }
   ],
   "source": [
    "#모델 학습 실행하기\n",
    "import time\n",
    "import copy\n",
    "\n",
    "def train_baseline(model, train_loader, val_loader, optimizer, num_epochs=30):\n",
    "    best_acc=0.0\n",
    "    best_model_wts=copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        since=time.time()\n",
    "        train(model, train_loader, optimizer)\n",
    "        train_loss, train_acc=evaluate(model, train_loader)\n",
    "        val_loss, val_acc=evaluate(model, val_loader)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc=val_acc\n",
    "            best_model_wts=copy.deepcopy(model.state_dict())\n",
    "        time_elapsed=time.time()-since\n",
    "        print(f\"epoch {epoch}\")\n",
    "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss,train_acc))\n",
    "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss,val_acc))\n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed //60, time_elapsed %60))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "base=train_baseline(model_base, train_loader,val_loader,optimizer,EPOCH)\n",
    "torch.save(base,'baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95471b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
